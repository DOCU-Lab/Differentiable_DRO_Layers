Review comments of “Control Policies With Guaranteed Robustness Against Input Perturbations Through Semidefinite
Constrained Reinforcement Learning”

This manuscript is concerned with training policies robust to observation perturbations in Reinforcement Learning. The manuscript is clear and easy to follow. 
However, the manuscript has some major issues that need to be addressed. My detailed comments are given as follows.

1. I think the biggest problem of this manuscript lies in that it is not clear why the determinant is used to handle the semidefinite constraint, i.e., 
from original problem (33) to (35), since the authors also admit that restricting the determinant does not lead to the semidefinite property. I would like
to see more theoretical intepretation to better motivate this approach.

2. The contribution of this paper lies in the application of the method in [39] to RL. However, given point 1, the proposed approach in ths manuscript
is not that convincing and I do not see any theoretical analysis through the manuscript, and the experiment is quite simple. I suggest the authors enhance the 
methodology part and/or the experiment part to strengthen the contribution. 

3. Cholesky decomposition is used as a suboracle in Algorithm 1, what is the complexity of this suboracle? How will the computational time grows as the neural 
network becomes larger (since the neural network used in the experiment is very small)?

4. In Algorithm 1, candidate update is accepted only when -M is semidefinite, and I believe this condition will slow down the training process. But in Fig 4, this
slow down does not happen, and I am wondering why it is this case (I guess a large L_{Bnd} is used). Since L_{Bnd} has a great affect on the training process, 
the author should draw the learning curve with different L_{Bnd} and compare it with that of the vanilla SAC method to better understand the pros and cons of this 
method.

5. In the numerical experiment, the authors does not compare with methods in previous research in RL that also aims at training a policy robust to observation 
perturbations, so I do not think the experiment part fully demonstrates the advantages of this method.